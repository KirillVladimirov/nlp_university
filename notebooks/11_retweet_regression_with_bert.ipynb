{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import traceback\n",
    "import datetime\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AdamW, BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigExperiment:\n",
    "    seed = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    embed_dim = 300\n",
    "    max_vocab_size = 50_000\n",
    "    batch_size = 128\n",
    "    num_epochs = 3\n",
    "    lr = 1e-2\n",
    "    num_workers = 0\n",
    "    patience = 5\n",
    "    early_stopping_delta = 2\n",
    "    save_dirname = \"models\"\n",
    "    \n",
    "config = ConfigExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    \n",
    "init_random_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train_processed_data_regression.csv\", index_col=False)\n",
    "validate = pd.read_csv(\"../data/validate_processed_data_regression.csv\", index_col=False)\n",
    "test = pd.read_csv(\"../data/test_processed_data_regression.csv\", index_col=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "TEXT = data.Field(batch_first=True,\n",
    "                  use_vocab=False,\n",
    "                  tokenize=tokenize_and_cut,\n",
    "                  preprocessing=tokenizer.convert_tokens_to_ids,\n",
    "                  init_token=init_token_idx,\n",
    "                  eos_token=eos_token_idx,\n",
    "                  pad_token=pad_token_idx,\n",
    "                  unk_token=unk_token_idx)\n",
    "# число всех сообщений пользователя в сети twitter;\n",
    "TSTCOUNT = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "# количество фоловеров пользователя (тех людей, которые читают пользователя);\n",
    "TFOLL = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "# количество друзей пользователя (те люди, которых читает пользователь);\n",
    "TFRIEN = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "# количество листов-подписок в которые добавлен твиттер-пользователь.\n",
    "LISTCOUNT = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "# число ретвитов\n",
    "TARGET = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "\n",
    "fields = [('text',TEXT), ('tstcount', TSTCOUNT), ('tfoll', TFOLL), ('tfrien', TFRIEN), ('listcount', LISTCOUNT), ('target', TARGET)]\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "    path=\"../data/\",\n",
    "    train=\"train_processed_data_regression.csv\",\n",
    "    validation=\"validate_processed_data_regression.csv\",\n",
    "    test=\"test_processed_data_regression.csv\",\n",
    "    format=\"csv\",\n",
    "    fields=fields,\n",
    "    skip_header=True)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key = lambda x: x.text,\n",
    "    batch_size=config.batch_size,\n",
    "    device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 128x54 (GPU 0)]\n",
      "\t[.tstcount]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.tfoll]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.tfrien]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.listcount]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.target]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "Valid:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 128x53 (GPU 0)]\n",
      "\t[.tstcount]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.tfoll]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.tfrien]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.listcount]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.target]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "Test:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 128]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 128x98 (GPU 0)]\n",
      "\t[.tstcount]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.tfoll]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.tfrien]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.listcount]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n",
      "\t[.target]:[torch.cuda.FloatTensor of size 128 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "for batch in train_iterator:\n",
    "    print(batch)\n",
    "    break\n",
    "    \n",
    "print('Valid:')\n",
    "for batch in valid_iterator:\n",
    "    print(batch)\n",
    "    break\n",
    "    \n",
    "print('Test:')\n",
    "for batch in test_iterator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultymodalNet(nn.Module):\n",
    "    def __init__(self, bert, n_filters, filter_sizes, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, \n",
    "                      out_channels=n_filters, \n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc_text = nn.Linear(len(filter_sizes) * n_filters, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim + 4, output_dim)\n",
    "\n",
    "    def forward(self, text, tstcount, tfoll, tfrien, listcount):\n",
    "        \n",
    "        tstcount, tfoll, tfrien, listcount = tstcount.unsqueeze(1), tfoll.unsqueeze(1), tfrien.unsqueeze(1), listcount.unsqueeze(1)\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.bert(text)[0]    \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        text_features = self.dropout(self.fc_text(cat))\n",
    "        \n",
    "        cat = torch.cat([text_features, tstcount, tfoll, tfrien, listcount], dim=1)\n",
    "\n",
    "        result = self.fc(cat)\n",
    "            \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILTERS = 128\n",
    "FILTER_SIZES = [2, 3]\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "model = MultymodalNet(bert, N_FILTERS, FILTER_SIZES, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 178,411,269 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 557,829 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convs.0.weight\n",
      "convs.0.bias\n",
      "convs.1.weight\n",
      "convs.1.bias\n",
      "fc_text.weight\n",
      "fc_text.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "to_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in to_decay)],\n",
    "        \"weight_decay_rate\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in to_decay)],\n",
    "        \"weight_decay_rate\": 0.00\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True, mode=\"max\", factor=0.3)\n",
    "\n",
    "model = model.to(config.device)\n",
    "criterion = criterion.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_dataloader: DataLoader, valid_dataloader: DataLoader, \n",
    "                 criterion, optimizer, scheduler, config: ConfigExperiment, model_name: str):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = config.device\n",
    "        self.config = config\n",
    "        self.threshold = 0.5\n",
    "        self.model_name = model_name\n",
    "        self.train_metrics = {\n",
    "            'avg_loss': [],\n",
    "            'mean_squared_error': [],\n",
    "            'mean_absolute_error': [],\n",
    "        }\n",
    "        self.valid_metrics = {\n",
    "            'avg_loss': [],\n",
    "            'mean_squared_error': [],\n",
    "            'mean_absolute_error': [],\n",
    "        }\n",
    "        self.counter = 0\n",
    "        self.delta = config.early_stopping_delta\n",
    "      \n",
    "    def run(self):\n",
    "        self.model.to(self.device)\n",
    "        best_valid_loss = float('inf')\n",
    "        best_target_metric = float('inf')\n",
    "\n",
    "        try:\n",
    "            for i_epoch in tqdm(range(self.config.num_epochs), desc='Epochs', total=config.num_epochs, position=1, leave=True):\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_loss, train_outputs, train_targets = self._train()\n",
    "                valid_loss, valid_outputs, valid_targets = self._evaluate()\n",
    "                    \n",
    "                self.train_metrics[\"avg_loss\"].append(train_loss)\n",
    "                self.train_metrics[\"mean_squared_error\"].append(mean_squared_error(train_targets, train_outputs.round()))\n",
    "                self.train_metrics[\"mean_absolute_error\"].append(mean_absolute_error(train_targets, train_outputs.round()))\n",
    "                \n",
    "                self.valid_metrics[\"avg_loss\"].append(valid_loss)\n",
    "                self.valid_metrics[\"mean_squared_error\"].append(mean_squared_error(valid_targets, valid_outputs.round()))\n",
    "                self.valid_metrics[\"mean_absolute_error\"].append(mean_absolute_error(valid_targets, valid_outputs.round()))\n",
    "                \n",
    "                end_time = time.time()\n",
    "                epoch_mins, epoch_secs = self._epoch_time(start_time, end_time)\n",
    "                self.print_progress(i_epoch, epoch_mins, epoch_secs)\n",
    "                \n",
    "                self.scheduler.step(self.valid_metrics[\"mean_squared_error\"][-1])\n",
    "                \n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    torch.save(model.state_dict(), f\"{config.save_dirname}/{self.model_name}.pth\")\n",
    "                    \n",
    "                if self.valid_metrics[\"mean_squared_error\"][-1] < best_target_metric:\n",
    "                    self.counter = 0\n",
    "                    best_target_metric = self.valid_metrics[\"mean_squared_error\"][-1]\n",
    "                    torch.save(model.state_dict(), f\"{config.save_dirname}/{self.model_name}.pth\")\n",
    "                else:\n",
    "                    self.counter += 1\n",
    "                    \n",
    "                if self.counter > self.config.patience:\n",
    "                    print(\"EarlyStopping\")\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        \n",
    "        return self.train_metrics, self.valid_metrics\n",
    "        \n",
    "    def _train(self):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_output = None\n",
    "        epoch_target = None\n",
    "        for i, batch in tqdm(enumerate(self.train_dataloader), desc='Train', total=len(self.train_dataloader), position=2, leave=True):\n",
    "            loss_iten, outputs = self._train_process(batch)\n",
    "            epoch_loss += loss_iten \n",
    "\n",
    "            if epoch_output is None:\n",
    "                epoch_output = outputs.cpu().data\n",
    "            else:\n",
    "                epoch_output = torch.cat((epoch_output, outputs.cpu().data))\n",
    "\n",
    "            if epoch_target is None:\n",
    "                epoch_target = batch.target.cpu().data\n",
    "            else:\n",
    "                epoch_target = torch.cat((epoch_target, batch.target.cpu().data))\n",
    "            \n",
    "        return epoch_loss / len(self.train_dataloader), epoch_output, epoch_target\n",
    "    \n",
    "    def _train_process(self, batch):      \n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(batch.text, batch.tstcount, batch.tfoll, batch.tfrien, batch.listcount).squeeze(1)\n",
    "        loss = self.criterion(outputs, batch.target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), outputs\n",
    "            \n",
    "    def _evaluate(self):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_output = None\n",
    "        epoch_target = None\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(enumerate(self.valid_dataloader), desc='Valid', total=len(self.valid_dataloader), position=3, leave=True):\n",
    "                outputs = self.model(batch.text, batch.tstcount, batch.tfoll, batch.tfrien, batch.listcount).squeeze(1)\n",
    "                loss = criterion(outputs, batch.target)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if epoch_output is None:\n",
    "                    epoch_output = outputs.cpu().data\n",
    "                else:\n",
    "                    epoch_output = torch.cat((epoch_output, outputs.cpu().data))\n",
    "\n",
    "                if epoch_target is None:\n",
    "                    epoch_target = batch.target.cpu().data\n",
    "                else:\n",
    "                    epoch_target = torch.cat((epoch_target, batch.target.cpu().data))\n",
    "\n",
    "        return epoch_loss / len(self.valid_dataloader), epoch_output, epoch_target\n",
    " \n",
    "    def _epoch_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "    def print_progress(self, i_epoch, epoch_mins, epoch_secs):\n",
    "        if type(i_epoch) != str:\n",
    "            i_epoch = i_epoch + 1\n",
    "            print(f\"Epoch: {i_epoch:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "            print(\"Training Results - Average Loss: {:.4f} | MSE: {:.4f} | MAE: {:.4f}\"\n",
    "                .format(\n",
    "                    self.train_metrics['avg_loss'][-1], \n",
    "                    self.train_metrics['mean_squared_error'][-1],\n",
    "                    self.train_metrics['mean_absolute_error'][-1],\n",
    "                ))      \n",
    "        else:\n",
    "            print(f\"Epoch: {i_epoch} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(\"Evaluating Results - Average Loss: {:.4f} | MSE: {:.4f} | MAE: {:.4f}\"\n",
    "            .format( \n",
    "                self.valid_metrics['avg_loss'][-1],\n",
    "                self.valid_metrics['mean_squared_error'][-1],\n",
    "                self.valid_metrics['mean_absolute_error'][-1],\n",
    "            ))\n",
    "        print()\n",
    "\n",
    "    def set_model(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        \n",
    "    def evaluate(self, dataloader: DataLoader):\n",
    "        self.valid_dataloader = dataloader\n",
    "        self.model.to(self.device)\n",
    "        start_time = time.time()\n",
    "\n",
    "        valid_loss, valid_outputs, valid_targets = self._evaluate()\n",
    "\n",
    "        self.valid_metrics[\"avg_loss\"].append(valid_loss)\n",
    "        self.valid_metrics[\"mean_squared_error\"].append(mean_squared_error(valid_targets, valid_outputs.round()))\n",
    "        self.valid_metrics[\"mean_absolute_error\"].append(mean_absolute_error(valid_targets, valid_outputs.round()))\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = self._epoch_time(start_time, end_time)\n",
    "        self.print_progress(\"evaluate\", epoch_mins, epoch_secs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f32b1c983fd4e83ac9b8d6b74af905f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=3.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fc610c7f1d48fc8587be1e50b3f16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1064.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826bbd2b9ebe4e3cb2a64c1dfdad43b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=355.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 01 | Time: 6m 41s\n",
      "Training Results - Average Loss: 89943.4997 | MSE: 90004.7812 | MAE: 31.8807\n",
      "Evaluating Results - Average Loss: 6618.0144 | MSE: 6628.3032 | MAE: 10.9924\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12cd2b618b24ef597a3d61e7e60a132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1064.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dddf339f50c4801bcb45451b85e93f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=355.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 02 | Time: 6m 42s\n",
      "Training Results - Average Loss: 9262.2286 | MSE: 9267.6963 | MAE: 14.2691\n",
      "Evaluating Results - Average Loss: 7073.5652 | MSE: 7085.0654 | MAE: 20.8669\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ab15db4b1e4e349b64e12b30e31aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=1064.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc3a2ff0f3d4861b52b236828258e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=355.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 03 | Time: 6m 47s\n",
      "Training Results - Average Loss: 9550.8689 | MSE: 9557.3848 | MAE: 13.9548\n",
      "Evaluating Results - Average Loss: 6189.9889 | MSE: 6200.1729 | MAE: 11.0799\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_iterator, valid_iterator, criterion, optimizer, scheduler, config, \"11_retweet_regression_with_bert\")\n",
    "trainer.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cac5fcc63ad419d885582f21beae43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=355.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: evaluate | Time: 1m 36s\n",
      "Evaluating Results - Average Loss: 13773.0130 | MSE: 13794.7168 | MAE: 12.1372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_iterator, valid_iterator, criterion, optimizer, scheduler, config, \"11_retweet_regression_with_bert\")\n",
    "model.load_state_dict(torch.load(f'{config.save_dirname}/11_retweet_regression_with_bert.pth'))\n",
    "trainer.set_model(model)\n",
    "trainer.evaluate(test_iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

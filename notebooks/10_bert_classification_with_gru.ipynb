{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import traceback\n",
    "import datetime\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, RandomSampler\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AdamW, BertForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigExperiment:\n",
    "    seed = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 32\n",
    "    num_epochs = 3\n",
    "    num_workers = 0\n",
    "    patience = 5\n",
    "    early_stopping_delta = 1e-4\n",
    "    save_dirname = \"models\"\n",
    "    \n",
    "config = ConfigExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    \n",
    "init_random_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train_processed_data.csv\", index_col=False)\n",
    "validate = pd.read_csv(\"../data/validate_processed_data.csv\", index_col=False)\n",
    "test = pd.read_csv(\"../data/test_processed_data.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>оставаться самый нужный и самый близкие ) весь...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>такой приятный чувство , когда ты знаешь , что...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>день начинаться с лень вообще ничто делать не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at_user at_user ксюша поход вплотную там суп з...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at_user с днём рождение at_user , творческий у...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  оставаться самый нужный и самый близкие ) весь...       1\n",
       "1  такой приятный чувство , когда ты знаешь , что...       1\n",
       "2  день начинаться с лень вообще ничто делать не ...       0\n",
       "3  at_user at_user ксюша поход вплотную там суп з...       1\n",
       "4  at_user с днём рождение at_user , творческий у...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(batch_first=True,\n",
    "                  use_vocab=False,\n",
    "                  tokenize=tokenize_and_cut,\n",
    "                  preprocessing=tokenizer.convert_tokens_to_ids,\n",
    "                  init_token=init_token_idx,\n",
    "                  eos_token=eos_token_idx,\n",
    "                  pad_token=pad_token_idx,\n",
    "                  unk_token=unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "\n",
    "fields = [('text',TEXT), ('label', LABEL)]\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "    path=\"../data/\",\n",
    "    train=\"train_processed_data.csv\",\n",
    "    validation=\"validate_processed_data.csv\",\n",
    "    test=\"test_processed_data.csv\",\n",
    "    format=\"csv\",\n",
    "    fields=fields,\n",
    "    skip_header=True)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key = lambda x: x.text,\n",
    "    batch_size=config.batch_size,\n",
    "    device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 136100\n",
      "Number of validation examples: 45367\n",
      "Number of testing examples: 45367\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [26676, 3248, 880, 852, 36700, 852, 851, 3248, 880, 852, 35637, 122, 11417, 3760, 6260, 852, 48752, 122, 851, 877, 4564, 29309, 122, 13496, 1997, 877, 14455, 3248, 1699, 24287, 36700, 852, 877, 3451, 3474, 8953, 122], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['оставаться', 'сам', '##ы', '##и', 'нужны', '##и', 'и', 'сам', '##ы', '##и', 'близкие', ')', 'весь', 'ост', '##ально', '##и', 'уходить', ')', 'и', 'я', 'только', 'рада', ')', 'потому', 'что', 'я', 'никогда', 'сам', 'не', 'понять', 'нужны', '##и', 'я', 'человек', 'или', 'нет', ')']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['text'])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len] \n",
    "\n",
    "        embedded = self.bert(text)[0]    \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])  \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 180,612,609 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,759,169 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "rnn.weight_ih_l1\n",
      "rnn.weight_hh_l1\n",
      "rnn.bias_ih_l1\n",
      "rnn.bias_hh_l1\n",
      "rnn.weight_ih_l1_reverse\n",
      "rnn.weight_hh_l1_reverse\n",
      "rnn.bias_ih_l1_reverse\n",
      "rnn.bias_hh_l1_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "to_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in to_decay)],\n",
    "        \"weight_decay_rate\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in to_decay)],\n",
    "        \"weight_decay_rate\": 0.00\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(config.device)\n",
    "criterion = criterion.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_dataloader: DataLoader, valid_dataloader: DataLoader, \n",
    "                 criterion, optimizer, scheduler, config: ConfigExperiment, model_name: str):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = config.device\n",
    "        self.config = config\n",
    "        self.threshold = 0.5\n",
    "        self.model_name = model_name\n",
    "        self.train_metrics = {\n",
    "            'avg_loss': [],\n",
    "            'accuracy': [],\n",
    "            'f1': [],\n",
    "        }\n",
    "        self.valid_metrics = {\n",
    "            'avg_loss': [],\n",
    "            'accuracy': [],\n",
    "            'f1': [],\n",
    "        }\n",
    "        self.counter = 0\n",
    "        self.delta = config.early_stopping_delta\n",
    "      \n",
    "    def run(self):\n",
    "        self.model.to(self.device)\n",
    "        best_valid_loss = float('inf')\n",
    "        best_target_metric = 0\n",
    "\n",
    "        try:\n",
    "            for i_epoch in tqdm(range(self.config.num_epochs), desc='Epochs', total=config.num_epochs, position=1, leave=True):\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_loss, train_outputs, train_targets = self._train()\n",
    "                valid_loss, valid_outputs, valid_targets = self._evaluate()\n",
    "                    \n",
    "                self.train_metrics[\"avg_loss\"].append(train_loss)\n",
    "                self.train_metrics[\"accuracy\"].append(accuracy_score(train_targets, train_outputs.round() > self.threshold))\n",
    "                self.train_metrics[\"f1\"].append(f1_score(train_targets, train_outputs.round() > self.threshold, average=\"macro\"))\n",
    "                \n",
    "                self.valid_metrics[\"avg_loss\"].append(valid_loss)\n",
    "                self.valid_metrics[\"accuracy\"].append(accuracy_score(valid_targets, valid_outputs.round() > self.threshold))\n",
    "                self.valid_metrics[\"f1\"].append(f1_score(valid_targets, valid_outputs.round() > self.threshold, average=\"macro\"))\n",
    "                \n",
    "                end_time = time.time()\n",
    "                epoch_mins, epoch_secs = self._epoch_time(start_time, end_time)\n",
    "                self.print_progress(i_epoch, epoch_mins, epoch_secs)\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step(self.valid_metrics[\"f1\"][-1])\n",
    "                \n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    torch.save(model.state_dict(), f\"{config.save_dirname}/{self.model_name}.pth\")\n",
    "                    \n",
    "                if self.valid_metrics[\"f1\"][-1] > best_target_metric:\n",
    "                    self.counter = 0\n",
    "                    best_target_metric = self.valid_metrics[\"f1\"][-1]\n",
    "                    torch.save(model.state_dict(), f\"{config.save_dirname}/{self.model_name}.pth\")\n",
    "                else:\n",
    "                    self.counter += 1\n",
    "                    \n",
    "                if self.counter > self.config.patience:\n",
    "                    print(\"EarlyStopping\")\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        \n",
    "        return self.train_metrics, self.valid_metrics\n",
    "        \n",
    "    def _train(self):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_output = None\n",
    "        epoch_target = None\n",
    "        for i, batch in tqdm(enumerate(self.train_dataloader), desc='Train', total=len(self.train_dataloader), position=2, leave=True):\n",
    "            loss_iten, outputs = self._train_process(batch)\n",
    "            epoch_loss += loss_iten \n",
    "\n",
    "            if epoch_output is None:\n",
    "                epoch_output = outputs.cpu().data\n",
    "            else:\n",
    "                epoch_output = torch.cat((epoch_output, outputs.cpu().data))\n",
    "\n",
    "            if epoch_target is None:\n",
    "                epoch_target = batch.label.cpu().data\n",
    "            else:\n",
    "                epoch_target = torch.cat((epoch_target, batch.label.cpu().data))\n",
    "            \n",
    "        return epoch_loss / len(self.train_dataloader), epoch_output, epoch_target\n",
    "    \n",
    "    def _train_process(self, batch):      \n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(batch.text).squeeze(1)\n",
    "        loss = self.criterion(outputs, batch.label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), outputs\n",
    "            \n",
    "    def _evaluate(self):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_output = None\n",
    "        epoch_target = None\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(enumerate(self.valid_dataloader), desc='Valid', total=len(self.valid_dataloader), position=3, leave=True):\n",
    "                outputs = self.model(batch.text).squeeze(1)\n",
    "                loss = criterion(outputs, batch.label)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if epoch_output is None:\n",
    "                    epoch_output = outputs.cpu().data\n",
    "                else:\n",
    "                    epoch_output = torch.cat((epoch_output, outputs.cpu().data))\n",
    "\n",
    "                if epoch_target is None:\n",
    "                    epoch_target = batch.label.cpu().data\n",
    "                else:\n",
    "                    epoch_target = torch.cat((epoch_target, batch.label.cpu().data))\n",
    "\n",
    "        return epoch_loss / len(self.valid_dataloader), epoch_output, epoch_target\n",
    " \n",
    "    def _epoch_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "    def print_progress(self, i_epoch, epoch_mins, epoch_secs):\n",
    "        if type(i_epoch) != str:\n",
    "            i_epoch = i_epoch + 1\n",
    "            print(f\"Epoch: {i_epoch:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "            print(\"Training Results - Average Loss: {:.4f} | accuracy: {:.4f} | f1: {:.4f}\"\n",
    "                .format(\n",
    "                    self.train_metrics['avg_loss'][-1], \n",
    "                    self.train_metrics['accuracy'][-1],\n",
    "                    self.train_metrics['f1'][-1],\n",
    "                ))      \n",
    "        else:\n",
    "            print(f\"Epoch: {i_epoch} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(\"Evaluating Results - Average Loss: {:.4f} | accuracy: {:.4f} | f1: {:.4f}\"\n",
    "            .format( \n",
    "                self.valid_metrics['avg_loss'][-1],\n",
    "                self.valid_metrics['accuracy'][-1],\n",
    "                self.valid_metrics['f1'][-1],\n",
    "            ))\n",
    "        print()\n",
    "\n",
    "    def set_model(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        \n",
    "    def evaluate(self, dataloader: DataLoader):\n",
    "        self.valid_dataloader = dataloader\n",
    "        self.model.to(self.device)\n",
    "        start_time = time.time()\n",
    "\n",
    "        valid_loss, valid_outputs, valid_targets = self._evaluate()\n",
    "\n",
    "        self.valid_metrics[\"avg_loss\"].append(valid_loss)\n",
    "        self.valid_metrics[\"accuracy\"].append(accuracy_score(valid_targets, valid_outputs.round() > self.threshold))\n",
    "        self.valid_metrics[\"f1\"].append(f1_score(valid_targets, valid_outputs.round() > self.threshold, average=\"macro\"))\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = self._epoch_time(start_time, end_time)\n",
    "        self.print_progress(\"evaluate\", epoch_mins, epoch_secs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862e02a4b55e40aeb18555f02f0fe926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=3.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bf201c289f401b94b55d7438656e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=4254.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fba73ecf6b472589790a36f43717ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=1418.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 01 | Time: 7m 25s\n",
      "Training Results - Average Loss: 0.1454 | accuracy: 0.9311 | f1: 0.9311\n",
      "Evaluating Results - Average Loss: 0.0603 | accuracy: 0.9726 | f1: 0.9726\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3a439db0054884b8226bfb746f848b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=4254.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8caa16bb6a1414fb9c8b63da5116ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=1418.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 02 | Time: 7m 29s\n",
      "Training Results - Average Loss: 0.0601 | accuracy: 0.9740 | f1: 0.9740\n",
      "Evaluating Results - Average Loss: 0.0417 | accuracy: 0.9813 | f1: 0.9813\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f07e35af7b44288a563ebc920c19654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=4254.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b1e932ebab4d129c0e82a8bc8ea413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=1418.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 03 | Time: 7m 33s\n",
      "Training Results - Average Loss: 0.0449 | accuracy: 0.9810 | f1: 0.9810\n",
      "Evaluating Results - Average Loss: 0.0304 | accuracy: 0.9873 | f1: 0.9873\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_iterator, valid_iterator, criterion, optimizer, None, config, \"10_bert_classification_with_gru\")\n",
    "trainer.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e40d8b438c42f7bbde15acb45e71e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=1418.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: evaluate | Time: 1m 36s\n",
      "Evaluating Results - Average Loss: 0.0302 | accuracy: 0.9874 | f1: 0.9874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_iterator, valid_iterator, criterion, optimizer, None, config, \"10_bert_classification_with_gru\")\n",
    "model.load_state_dict(torch.load(f'{config.save_dirname}/10_bert_classification_with_gru.pth'))\n",
    "trainer.set_model(model)\n",
    "trainer.evaluate(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 7.93 GiB total capacity; 2.69 GiB already allocated; 353.88 MiB free; 2.82 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-af6b0cf876e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTGRUSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBIDIRECTIONAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROPOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{config.save_dirname}/11_bert_classification.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 7.93 GiB total capacity; 2.69 GiB already allocated; 353.88 MiB free; 2.82 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "model.load_state_dict(torch.load(f'{config.save_dirname}/10_bert_classification_with_gru.pth'))\n",
    "\n",
    "for name, param in list(model.named_parameters())[150:]:                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = True\n",
    "        \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "to_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in to_decay)],\n",
    "        \"weight_decay_rate\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in to_decay)],\n",
    "        \"weight_decay_rate\": 0.00\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(config.device)\n",
    "criterion = criterion.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 4\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key=lambda x: x.text,\n",
    "    batch_size=config.batch_size,\n",
    "    device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3e80a53d854126bc79cbf69de46a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=3.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3a6b7523bc41619f914548d19347f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=34025.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 7.93 GiB total capacity; 2.69 GiB already allocated; 353.88 MiB free; 2.82 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f4ebb66f536 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7f4ebb8b8f1e in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7f4ebb8b9f9e in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x7f4ebe4479e5 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf688bb (0x7f4ebca338bb in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfb21a7 (0x7f4ebca7d1a7 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1073c49 (0x7f4ef9350c49 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x1073f87 (0x7f4ef9350f87 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe1ff1e (0x7f4ef90fcf1e in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::native::zeros(c10::ArrayRef<long>, c10::TensorOptions const&) + 0x2d (0x7f4ef91017dd in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x113f0f3 (0x7f4ef941c0f3 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x1074339 (0x7f4ef9351339 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x1071922 (0x7f4ef934e922 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: <unknown function> + 0x1074339 (0x7f4ef9351339 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::native::embedding_dense_backward_cuda(at::Tensor const&, at::Tensor const&, long, long, bool) + 0xafa (0x7f4ebdef1e7a in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #15: <unknown function> + 0xf6ff2c (0x7f4ebca3af2c in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0x10c42fc (0x7f4ef93a12fc in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: <unknown function> + 0x2aa51d1 (0x7f4efad821d1 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x10c42fc (0x7f4ef93a12fc in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: at::native::embedding_backward(at::Tensor const&, at::Tensor const&, long, long, bool, bool) + 0x124 (0x7f4ef8eed954 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x114f290 (0x7f4ef942c290 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x2c4e76a (0x7f4efaf2b76a in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: <unknown function> + 0x1187b89 (0x7f4ef9464b89 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::generated::EmbeddingBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1cd (0x7f4efab8497d in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: <unknown function> + 0x2d89705 (0x7f4efb066705 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f4efb063a03 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f4efb0647e2 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #27: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f4efb05ce59 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #28: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f4f079a05f8 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\nframe #29: <unknown function> + 0xbd6df (0x7f4f7a2506df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #30: <unknown function> + 0x76db (0x7f4f824d46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #31: clone + 0x3f (0x7f4f81a58a3f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3bf08662bb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"11_bert_classification_v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-d464ac761334>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d464ac761334>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mepoch_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mloss_iten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_iten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d464ac761334>\u001b[0m in \u001b[0;36m_train_process\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nlp_university/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 7.93 GiB total capacity; 2.69 GiB already allocated; 353.88 MiB free; 2.82 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f4ebb66f536 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7f4ebb8b8f1e in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7f4ebb8b9f9e in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x7f4ebe4479e5 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf688bb (0x7f4ebca338bb in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfb21a7 (0x7f4ebca7d1a7 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1073c49 (0x7f4ef9350c49 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x1073f87 (0x7f4ef9350f87 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe1ff1e (0x7f4ef90fcf1e in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::native::zeros(c10::ArrayRef<long>, c10::TensorOptions const&) + 0x2d (0x7f4ef91017dd in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x113f0f3 (0x7f4ef941c0f3 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x1074339 (0x7f4ef9351339 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x1071922 (0x7f4ef934e922 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: <unknown function> + 0x1074339 (0x7f4ef9351339 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::native::embedding_dense_backward_cuda(at::Tensor const&, at::Tensor const&, long, long, bool) + 0xafa (0x7f4ebdef1e7a in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #15: <unknown function> + 0xf6ff2c (0x7f4ebca3af2c in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0x10c42fc (0x7f4ef93a12fc in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: <unknown function> + 0x2aa51d1 (0x7f4efad821d1 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x10c42fc (0x7f4ef93a12fc in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: at::native::embedding_backward(at::Tensor const&, at::Tensor const&, long, long, bool, bool) + 0x124 (0x7f4ef8eed954 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x114f290 (0x7f4ef942c290 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x2c4e76a (0x7f4efaf2b76a in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: <unknown function> + 0x1187b89 (0x7f4ef9464b89 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::generated::EmbeddingBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1cd (0x7f4efab8497d in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: <unknown function> + 0x2d89705 (0x7f4efb066705 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f4efb063a03 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #26: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f4efb0647e2 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #27: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f4efb05ce59 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\nframe #28: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f4f079a05f8 in /home/science/projects/nlp_university/venv/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\nframe #29: <unknown function> + 0xbd6df (0x7f4f7a2506df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #30: <unknown function> + 0x76db (0x7f4f824d46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #31: clone + 0x3f (0x7f4f81a58a3f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_iterator, valid_iterator, criterion, optimizer, None, config, \"10_bert_classification_with_gru\")\n",
    "trainer.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb63015271c748338821f1e0b467e8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Valid', max=2836.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: evaluate | Time: 1m 42s\n",
      "Evaluating Results - Average Loss: 0.0204 | accuracy: 0.9912 | f1: 0.9912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_iterator, valid_iterator, criterion, optimizer, None, config, \"10_bert_classification_with_gru\")\n",
    "model.load_state_dict(torch.load(f'{config.save_dirname}/10_bert_classification_with_gru.pth'))\n",
    "trainer.set_model(model)\n",
    "trainer.evaluate(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

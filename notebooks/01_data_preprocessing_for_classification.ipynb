{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "positive_file = \"../data/positive.csv\"\n",
    "negative_file = \"../data/negative.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2d59dd2dc01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0minit_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2d59dd2dc01a>\u001b[0m in \u001b[0;36minit_random_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mSetup\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"id\", \"tdate\", \"tmane\", \"ttext\", \"ttype\", \"trep\", \"trtw\", \"tfav\", \"tstcount\", \"tfoll\", \"tfrien\", \"listcount\"]\n",
    "positive_df = pd.read_csv(positive_file, sep=\";\", names=column_names, index_col=False)\n",
    "negative_df = pd.read_csv(negative_file, sep=\";\", names=column_names, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смена метки класса для отрицательной эмоциональной окраски\n",
    "negative_df[\"ttype\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([negative_df, positive_df])\n",
    "df.shape, negative_df.shape, positive_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"ttext\", \"ttype\"]]\n",
    "df.columns = ['text', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стадии очистки текста:\n",
    "1. Приведение к нижниму регистру\n",
    "2. Замена буквы \"ё\" на \"е\"\n",
    "3. Удаление цифр\n",
    "4. Удаление HTML специальных символов\n",
    "5. Замена упоминаний пользователей @username на тег at_user\n",
    "6. Замена символа хештега # на тег hash\n",
    "7. Удаление RT\n",
    "8. Замена гиперссылок на тег url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе \"Sentiment Analysis of Posts and Comments in the Accounts of Russian Politicians on the Social Network\"\n",
    "в разделе \"III. DATA AND METHODOLOGY\" рассматривается стадия распознования смайликов в последовательностях пунктуации \n",
    "и замену их на теги соответствующих смайлов. Это уменьшит \"шум\", создоваемый черезмерным употреблением символов пунктуации\n",
    "\n",
    "Для простоты, я не стал удалять знаки пунктуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    # Remove digits\n",
    "    text = re.sub(\"\\d+:\\d+\", \" \", text)\n",
    "    text = re.sub(\" \\d+\", \" \", text)\n",
    "    # Removing ;quot; and &amp\n",
    "    text = re.sub(';quot;', ' ', text) \n",
    "    text = re.sub('&amp', ' ', text) \n",
    "    # Remove HTML special entities \n",
    "    text = re.sub(r'\\&\\w*;', ' ', text)\n",
    "    #Convert @username to AT_USER\n",
    "    text = re.sub('@[^\\s]+','at_user', text)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    # Removing '#' hash tag\n",
    "    text = re.sub('#', 'hash ', text) \n",
    "    # Removing RT\n",
    "    text = re.sub('rt[\\s]+', '', text) \n",
    "    # Removing hyperlink\n",
    "    text = re.sub('https?:\\/\\/\\S+', 'url', text)\n",
    "    # Separate words and punctuation\n",
    "    text = re.findall(r\"[\\w']+|[.,!?;:()]\", text)\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweets\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использую лемматизатор от pymorphy2. Он не снимает омонимию, так как обрабатывате каждое сло по отдельности.\n",
    "Для обычных текстов я бы использовал pymystem3. Он обрабатывает предложения целиком и способен различить разные слова с одинаковым написанием."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = []\n",
    "    for token in text.split():\n",
    "        # Если токен уже был закеширован, быстро возьмем результат из кэша.\n",
    "        if token in cache.keys():\n",
    "            words.append(cache[token])\n",
    "        # Слово еще не встретилось, будем проводить медленный морфологический анализ.\n",
    "        else:\n",
    "            result = morph.parse(token)   \n",
    "            word = result[0].normal_form\n",
    "            # Отправляем слово в результат, ...\n",
    "            words.append(word)\n",
    "            # ... и кешируем результат его разбора.\n",
    "            cache[token] = word   \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['text'].map(str) == 'nan'].index)\n",
    "\n",
    "train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "train.to_csv(\"../data/train_processed_data.csv\", index=False)\n",
    "validate.to_csv(\"../data/validate_processed_data.csv\", index=False)\n",
    "test.to_csv(\"../data/test_processed_data.csv\", index=False)\n",
    "\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

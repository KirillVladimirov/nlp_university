{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import traceback\n",
    "import datetime\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigExperiment:\n",
    "    seed = 42\n",
    "    test_size = 0.3\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    embed_dim = 300\n",
    "    max_vocab_size = 50_000\n",
    "    batch_size = 64\n",
    "    num_epochs = 30\n",
    "    lr = 1e-2\n",
    "    num_workers = 0\n",
    "    patience = 3\n",
    "    early_stopping_delta = 1e-4\n",
    "    save_dirname = \"models\"\n",
    "    \n",
    "config = ConfigExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    \n",
    "init_random_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136100, 2), (45367, 2), (45367, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train_processed_data.csv\", index_col=False)\n",
    "validate = pd.read_csv(\"../data/validate_processed_data.csv\", index_col=False)\n",
    "test = pd.read_csv(\"../data/test_processed_data.csv\", index_col=False)\n",
    "\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание полей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: str(x).split()\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenize, batch_first=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "fields = [('text',TEXT), ('label', LABEL)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание датасета из файлов csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path=\"../data/\",\n",
    "                                        train=\"train_processed_data.csv\",\n",
    "                                        validation=\"validate_processed_data.csv\",\n",
    "                                        test=\"test_processed_data.csv\",\n",
    "                                        format=\"csv\",\n",
    "                                        fields=fields,\n",
    "                                        skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['оставаться', 'самый', 'нужный', 'и', 'самый', 'близкие', ')', 'весь', 'остальной', 'уходить', ')', 'и', 'я', 'только', 'рада', ')', 'потому', 'что', 'я', 'никогда', 'сам', 'не', 'понять', 'нужный', 'я', 'человек', 'или', 'нет', ')'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['at_user', 'at_user', 'я', 'старушка', '(', '(', '('], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(valid_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['at_user', 'привееть', ',', 'хелена', ':', ')', ')', ')', 'мимими', '.', '.', '.', 'пряник', ',', 'конфета', 'и', 'сирец'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 136100\n",
      "Number of validation examples: 45367\n",
      "Number of testing examples: 45367\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построить словарь "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, min_freq=2)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 33041\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создать объекты итераторов для каждого датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key = lambda x: x.text,\n",
    "    batch_size=config.batch_size,\n",
    "    device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 64x32 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "Valid:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 64x46 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n",
      "Test:\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64]\n",
      "\t[.text]:[torch.cuda.LongTensor of size 64x95 (GPU 0)]\n",
      "\t[.label]:[torch.cuda.FloatTensor of size 64 (GPU 0)]\n"
     ]
    }
   ],
   "source": [
    "print('Train:')\n",
    "for batch in train_iterator:\n",
    "    print(batch)\n",
    "    break\n",
    "    \n",
    "print('Valid:')\n",
    "for batch in valid_iterator:\n",
    "    print(batch)\n",
    "    break\n",
    "    \n",
    "print('Test:')\n",
    "for batch in test_iterator:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Самые частые слова в текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', 126857), (')', 116605), (',', 112838), ('.', 110646), ('at_user', 89459), (':', 64837), ('я', 64792), ('не', 44975), ('!', 39989), ('и', 36467), ('в', 35466), ('что', 23049), ('на', 22897), ('а', 21843), ('?', 21683), ('url', 20083), ('с', 20052), ('весь', 18848), ('ты', 18175), ('быть', 16970)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словарь для текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '(', ')', ',', '.', 'at_user', ':', 'я', 'не']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Словарь меток классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

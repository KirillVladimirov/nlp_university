{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pymorphy2\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "positive_file = \"../data/positive.csv\"\n",
    "negative_file = \"../data/negative.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"id\", \"tdate\", \"tmane\", \"ttext\", \"ttype\", \"trep\", \"trtw\", \"tfav\", \"tstcount\", \"tfoll\", \"tfrien\", \"listcount\"]\n",
    "positive_df = pd.read_csv(positive_file, sep=\";\", names=column_names, index_col=False)\n",
    "negative_df = pd.read_csv(negative_file, sep=\";\", names=column_names, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смена метки класса для отрицательной эмоциональной окраски\n",
    "negative_df[\"ttype\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((226834, 12), (111923, 12), (114911, 12))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([negative_df, positive_df])\n",
    "df.shape, negative_df.shape, positive_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"ttext\", \"ttype\"]]\n",
    "df.columns = ['text', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на работе был полный пиддес :| и так каждое закрытие месяца, я же свихнусь так D:',\n",
       " 'Коллеги сидят рубятся в Urban terror, а я из-за долбанной винды не могу :(',\n",
       " '@elina_4post как говорят обещаного три года ждут...((',\n",
       " 'Желаю хорошего полёта и удачной посадки,я буду очень сильно скучать( http://t.co/jCLNzVNv3S',\n",
       " 'Обновил за каким-то лешим surf, теперь не работает простоплеер :(',\n",
       " 'Котёнка вчера носик разбила, плакала и расстраивалась :(',\n",
       " '@juliamayko @O_nika55 @and_Possum Зашли, а то он опять затихарился, я прямо физически страдаю, когда он долго молчит!(((',\n",
       " 'а вообще я не болею -  я не выздоравливаю :(',\n",
       " 'я микрофраза :( учимся срать кирпичами в режиме &amp;quot;нон-стоп&amp;quot; @niwoqisipapy',\n",
       " 'я хочу с тобой помириться , но сука я гордая и никогда этого не сделаю! (((',\n",
       " '@DNO_OKEANA_A3A3 @MOE_MOPE_A3A3 тебя ебет какие у меня фотки.я про твои молчу.и вообще ты хоть знаешь как ТП то выглядят...',\n",
       " 'Блин начали сниться сны( Не когда почти не снились ! А теперь можно сказать каждый день такое ! Мне это не нравиться ((((((((((',\n",
       " '@realVold На твоем месте, я бы сначала телек купил бы(',\n",
       " '@faraonbgr111 плохо боишься значит(( ты где? В мск бгр?',\n",
       " 'А я хотела... электромобиль =(\\nhttp://t.co/vgs9JeUPHz',\n",
       " 'Скоро увижу твои зеленые глаза в последний раз(((',\n",
       " 'Вот это да, докатился,по-оранжевел патриот! Чо деется? :(( RT @SapelSV: #Бабки объединят #КПРФ и Навального? http://t.co/kyJk4A79kv',\n",
       " 'Ну уже 12:25 ночииии:(((( а я ничего не сделалаллалалалалалалвцмдэмппдпктцдкиицкп',\n",
       " '@190299der я очень рада,но ты им разобьешь сердца:(\\nМне их  жалко епт\\nмне оч жалко Влада\\nА всех остальных нет\\n',\n",
       " 'Сегодня какой-то нехороший день у меня :-( Телефон наушники не опознает, у блога как-то умудрился шаблон сломать, да еще',\n",
       " 'думаю,на моем веку почта россии не поднимется со дна (((',\n",
       " 'Мне одному   напоминает дом-2? Путин делает реалити-шоу? O_o',\n",
       " 'ебааааа сейчас модуль будет по региональной экономике ((',\n",
       " '@kordalona_mars оооууу ты меня анфолловил чувак((',\n",
       " 'вот так я встречаю зиму:(( http://t.co/FjivXAGcR9',\n",
       " '@Kyo_oppa ну вот, туда медведь прикреплялся.',\n",
       " 'чорт... хотела сходить в кино...  придется любоваться фильмом в уг качестве на компе(',\n",
       " 'RT @arinaaaaonskul: Так плохо я себя не чувствовала давно :( ',\n",
       " 'Как понизить температуру ноута? Снаружи +30, так что он постоянно раскалённый :(',\n",
       " '@Anastasia_pain думаешь упаду со стула? :(',\n",
       " '@Deniska_perm да нет, но условия будут более спартанские :(',\n",
       " 'Хотела маме сделать сюрприз, как всегда все навернулось(',\n",
       " 'я блин сутками его жду а он с друзьями общаться! с которыми ежедневно видится! :(',\n",
       " '@Annya_Nik @YankaMurygina Так и думал:( \\nНе судьба походу попасть к мастерам',\n",
       " 'RT @davidmsk: У берегов Коста-Рики водятся 1600 видов рыб. А на берег выходят черепахи весом до 500 кг. Но я не видел :( http://t.co/9ixMBH…',\n",
       " '\"@lisi_Ca: @maybe_reg49_27 @One_ENCLAVE @LyudaLa самая смешная из лисичек - тибетская. Это не фотошоп http://t.co/kDDYZartwl\" о_О',\n",
       " 'Потеряла креапления от борда. Как так! Я же их не снимала(',\n",
       " 'RT @Ksenyamalischev: Устала от семинара;(',\n",
       " 'Опять простыла....!!!;(Все гуляют,катают,тусуют,а я!;(((Что за невезение!!!',\n",
       " '@1libertad первые деньги пойдут на красные мокасины, потом на ремень гермес, потом на приору, а потом уже можно умереть :-/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    # Remove digits\n",
    "    text = re.sub(\"\\d+:\\d+\", \" \", text)\n",
    "    text = re.sub(\" \\d+\", \" \", text)\n",
    "    # Removing ;quot; and &amp\n",
    "    text = re.sub(';quot;', ' ', text) \n",
    "    text = re.sub('&amp', ' ', text) \n",
    "    # Remove HTML special entities \n",
    "    text = re.sub(r'\\&\\w*;', ' ', text)\n",
    "    #Convert @username to AT_USER\n",
    "    text = re.sub('@[^\\s]+','at_user', text)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    # Removing '#' hash tag\n",
    "    text = re.sub('#', 'hash ', text) \n",
    "    # Removing RT\n",
    "    text = re.sub('rt[\\s]+', '', text) \n",
    "    # Removing hyperlink\n",
    "    text = re.sub('https?:\\/\\/\\S+', 'url', text)\n",
    "    # Separate words and punctuation\n",
    "    text = re.findall(r\"[\\w']+|[.,!?;:()]\", text)\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweets\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = []\n",
    "    for token in text.split():\n",
    "        # Если токен уже был закеширован, быстро возьмем результат из кэша.\n",
    "        if token in cache.keys():\n",
    "            words.append(cache[token])\n",
    "        # Слово еще не встретилось, будем проводить медленный морфологический анализ.\n",
    "        else:\n",
    "            result = morph.parse(token)   \n",
    "            word = result[0].normal_form\n",
    "            # Отправляем слово в результат, ...\n",
    "            words.append(word)\n",
    "            # ... и кешируем результат его разбора.\n",
    "            cache[token] = word   \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.5 s, sys: 100 ms, total: 43.6 s\n",
      "Wall time: 43.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на работа быть полный пиддес : и так каждый закрытие месяц , я же свихнуться так d :',\n",
       " 'коллега сидеть рубиться в urban terror , а я из за долбать винд не мочь : (',\n",
       " 'at_user как говорят обещаной три год ждать . . . ( (',\n",
       " 'желать хороший полёт и удачный посадка , я быть очень сильно скучать ( url',\n",
       " 'обновить за какой то леший surf , теперь не работать простоплеер : (',\n",
       " 'котёнок вчера носик разбить , плакать и расстраиваться : (',\n",
       " 'at_user at_user at_user заслать , а то он опять затихариться , я прямо физически страдать , когда он долго молчать ! ( ( (',\n",
       " 'а вообще я не болеть я не выздоравливать : (',\n",
       " 'я микрофраза : ( учиться срать кирпич в режим нона стоп at_user',\n",
       " 'я хотеть с ты помириться , но сук я гордый и никогда это не сделать ! ( ( (',\n",
       " 'at_user at_user ты ебета какой у я фотка . я про твой молчать . и вообще ты хоть знаешь как тп то выглядеть . . .',\n",
       " 'блин начать сниться сон ( не когда почти не сниться ! а теперь можно сказать каждый день такой ! я это не нравиться ( ( ( ( ( ( ( ( ( (',\n",
       " 'at_user на твой место , я бы сначала телек купить бы (',\n",
       " 'at_user плохо бояться значит ( ( ты где ? в мск бгр ?',\n",
       " 'а я хотеть . . . электромобиль ( url',\n",
       " 'скоро увидеть твой зелёный глаз в последний раз ( ( (',\n",
       " 'вот это да , докатиться , по оранжевело патриот ! чо деяться ? : ( ( at_user hash бабка объединить hash кпрф и навальный ? url',\n",
       " 'ну уже ночииия : ( ( ( ( а я ничто не сделалаллалалалалалалвцмдэмппдпктцдкиицкп',\n",
       " 'at_user я очень рада , но ты имя разбить сердце : ( я они жалко епт я оч жалко влад а весь остальной нет',\n",
       " 'сегодня какой то нехороший день у я : ( телефон наушник не опознать , у блог как то умудриться шаблон сломать , да ещё',\n",
       " 'думать , на мыть век почта россия не подняться с дно ( ( (',\n",
       " 'я один напоминать дом 2 ? путин делать реалить шоу ? o_o',\n",
       " 'ебааааа сейчас модуль быть по региональный экономика ( (',\n",
       " 'at_user оооууа ты я анфолловить чувак ( (',\n",
       " 'вот так я встречать зима : ( ( url',\n",
       " 'at_user ну вот , туда медведь прикрепляться .',\n",
       " 'чорт . . . хотеть сходить в кино . . . прийтись любоваться фильм в уг качество на комп (',\n",
       " 'at_user так плохо я себя не чувствовать давно : (',\n",
       " 'как понизить температура ноут ? снаружи 30 , так что он постоянно раскалить : (',\n",
       " 'at_user думать упад с стул ? : (',\n",
       " 'at_user да нет , но условие быть более спартанский : (',\n",
       " 'хотеть мама сделать сюрприз , как всегда весь навернуться (',\n",
       " 'я блин сутки он ждать а он с друг общаться ! с который ежедневно видеться ! : (',\n",
       " 'at_user at_user так и думать : ( не судьба поход попасть к мастер',\n",
       " 'at_user у берег коста рик водиться вид рыба . а на берег выходить черепаха вес до килограмм . но я не видеть : ( url',\n",
       " 'at_user at_user at_user at_user самый смешной из лисичка тибетский . это не фотошоп url о_о',\n",
       " 'потерять креапление от борд . как так ! я же они не снимать (',\n",
       " 'at_user устать от семинар ; (',\n",
       " 'опять простыть . . . . ! ! ! ; ( весь гулять , катать , тусовать , а я ! ; ( ( ( что за невезение ! ! !',\n",
       " 'at_user один деньга пойти на красный мокасин , потом на ремень гермес , потом на приор , а потом уже можно умереть :']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"].tolist(), df[\"target\"].tolist(), \n",
    "    test_size=0.33, \n",
    "    random_state=42, \n",
    "    shuffle=True, \n",
    "    stratify=df[\"target\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in tweet.split():\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39717, 32555)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs[('я', 0)], freqs[('я', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 2)) \n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in tweet.split():\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,0] += freqs.get((word, 1)) if freqs.get((word, 1)) else 0\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,1] += freqs.get((word, 0)) if freqs.get((word, 0)) else 0\n",
    "        \n",
    "    assert(x.shape == (1, 2))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X_train_features = np.zeros((len(X_train), 2))\n",
    "for i in range(len(X_train)):\n",
    "    X_train_features[i, :]= extract_features(X_train[i], freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 269364.,  369324.],\n",
       "       [ 421216.,  264586.],\n",
       "       [ 185628.,  208637.],\n",
       "       [ 350531.,  206837.],\n",
       "       [ 366061.,   73040.],\n",
       "       [ 167601.,  282624.],\n",
       "       [  19351., 1863304.],\n",
       "       [ 346173.,  312400.],\n",
       "       [ 783480.,  397093.],\n",
       "       [ 280892.,  391066.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X_test_features = np.zeros((len(X_test), 2))\n",
    "for i in range(len(X_test)):\n",
    "    X_test_features[i, :]= extract_features(X_test[i], freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "        \"classifier\": [LogisticRegression()],\n",
    "        \"classifier__penalty\": ['l2','l1'],\n",
    "        \"classifier__C\": np.logspace(0, 4, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=20)]: Done 100 out of 100 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: 0.9731626948339157 using {'classifier': LogisticRegression(), 'classifier__C': 1.0, 'classifier__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(pipeline, cv=5, param_grid=parameters, verbose=1, n_jobs=20, scoring=\"f1\")\n",
    "grid.fit(X_train_features, y_train)\n",
    "print(f\"Best Model: {grid.best_score_} using {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9724003419899541\n",
      "\n",
      "\n",
      "confusion matrix: \n",
      " [[35335  1600]\n",
      " [  466 37455]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97     36935\n",
      "           1       0.96      0.99      0.97     37921\n",
      "\n",
      "    accuracy                           0.97     74856\n",
      "   macro avg       0.97      0.97      0.97     74856\n",
      "weighted avg       0.97      0.97      0.97     74856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save best model to current working directory\n",
    "joblib.dump(grid, \"models/02_sklearn_pipeline_linear_classifier.pkl\")\n",
    "# load from file and predict using the best configs found in the CV step\n",
    "model = joblib.load(\"models/02_sklearn_pipeline_linear_classifier.pkl\" )\n",
    "# get predictions from best model above\n",
    "y_preds = model.predict(X_test_features)\n",
    "print('accuracy score: ', accuracy_score(y_test, y_preds))\n",
    "print('\\n')\n",
    "print('confusion matrix: \\n', confusion_matrix(y_test,y_preds))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

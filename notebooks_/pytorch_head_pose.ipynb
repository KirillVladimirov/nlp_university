{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from typing import Callable, List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  00_efficientnet-b0  Created \n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"00_efficientnet-b0\"\n",
    "\n",
    "class ConfigExperiment:\n",
    "    logdir = f\"./logs/{EXPERIMENT_NAME}\"\n",
    "    save_dirname = EXPERIMENT_NAME\n",
    "    submission_file = f\"{EXPERIMENT_NAME}.csv\"\n",
    "    seed = 42\n",
    "    batch_size = 8\n",
    "    model_name = 'efficientnet-b0'\n",
    "    size = 512\n",
    "    num_workers = 20\n",
    "    root_images = \"\"\n",
    "    root = \"\"\n",
    "    num_classes = 0\n",
    "    patience = 10\n",
    "    early_stopping_delta = 1e-4\n",
    "    num_epochs = 200\n",
    "    lr = 0.003\n",
    "    class_names = []\n",
    "    is_fp16_used = False\n",
    "    \n",
    "    \n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    \n",
    "config = ConfigExperiment()\n",
    "set_seed(config.seed)\n",
    "config.size = EfficientNet.get_image_size(config.model_name)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(config.save_dirname)\n",
    "    print(\"Directory \" , config.save_dirname ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , config.save_dirname ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2886d2ff15ad45f8ae5de56ab37ca43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2790.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2790 (2790, 2)\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "# 1. Нужно составить список изображений и их описаний \n",
    "for image_name in tqdm(glob.glob('HeadPoseImageDatabase/Person*/*.jpg', recursive=True)):\n",
    "    data = {}\n",
    "    data[\"class_name\"] = os.path.splitext(image_name)[0][-6:]\n",
    "    data[\"file_name\"] = image_name\n",
    "    dataset.append(data)\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame(dataset)\n",
    "print(df.file_name.unique().shape[0], df.shape)\n",
    "print(df.class_name.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2232, 95) (558, 95)\n"
     ]
    }
   ],
   "source": [
    "# 2. Разбить список изображений на test и trian \n",
    "config.num_classes = len(df.class_name.unique().tolist())\n",
    "config.class_names = df.class_name.unique().tolist()\n",
    "df = pd.concat((df, pd.get_dummies(df[\"class_name\"])), axis=1)\n",
    "train, valid = train_test_split(df, test_size=0.2, shuffle=True, random_state=config.seed, stratify=df[config.class_names]) \n",
    "print(train.shape, valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadPoseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, config, transforms=None):\n",
    "    \n",
    "        self.df = df\n",
    "        self.images_dir = config.root_images\n",
    "        self.class_names = config.class_names\n",
    "        self.transforms=transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_src = self.images_dir + self.df.iloc[idx]['file_name']\n",
    "        image = cv2.imread(image_src, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        labels = self.df.iloc[idx][self.class_names].values.astype(np.int8)\n",
    "        label = torch.argmax(torch.from_numpy(labels))\n",
    "        \n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image)\n",
    "            image = transformed['image']\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_transforms(image_size=224):\n",
    "    # Convert the image to a square of size image_size x image_size\n",
    "    # (keeping aspect ratio)\n",
    "    result = [\n",
    "        A.LongestMaxSize(max_size=image_size),\n",
    "        A.PadIfNeeded(image_size, image_size, border_mode=0)\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def hard_transforms():\n",
    "    result = [\n",
    "        A.OneOf([\n",
    "            A.IAAEmboss(p=1.0),\n",
    "            A.IAASharpen(p=1.0),\n",
    "            A.Blur(p=1.0),\n",
    "        ], p=0.5),\n",
    "\n",
    "        # Affine\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(p=1.0),\n",
    "            A.IAAPiecewiseAffine(p=1.0)\n",
    "        ], p=0.5),\n",
    "    ]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def post_transforms():\n",
    "    # we use ImageNet image normalization\n",
    "    # and convert it to torch.Tensor\n",
    "    return [A.Normalize(p=1.0), ToTensorV2(p=1.0),]\n",
    "\n",
    "def compose(transforms_to_compose):\n",
    "    # combine all augmentations into one single pipeline\n",
    "    result = A.Compose([item for sublist in transforms_to_compose for item in sublist])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = compose([\n",
    "    pre_transforms(config.size),\n",
    "#     hard_transforms(), \n",
    "    post_transforms()\n",
    "])\n",
    "valid_transforms = compose([\n",
    "    pre_transforms(config.size), \n",
    "    post_transforms()\n",
    "])\n",
    "\n",
    "show_transforms = compose([\n",
    "    pre_transforms(config.size),\n",
    "#     hard_transforms()\n",
    "])\n",
    "\n",
    "train_dataset = HeadPoseDataset(train, config, train_transforms)\n",
    "valid_dataset = HeadPoseDataset(valid, config, valid_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "def get_model(model_name: str, num_classes: int, pretrained: str = \"imagenet\") -> EfficientNet:\n",
    "    model = EfficientNet.from_pretrained(model_name)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model._fc.in_features\n",
    "    model._fc = nn.Sequential(nn.Linear(num_ftrs, num_classes, bias = True))\n",
    "    return model\n",
    "\n",
    "model = get_model(config.model_name, config.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True, mode=\"max\", factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_dataloader: DataLoader, valid_dataloader: DataLoader, criterion, optimizer, scheduler, device, config: ConfigExperiment):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.train_metrics = {\n",
    "            'avg_loss': [],\n",
    "            'accuracy': [],\n",
    "        }\n",
    "        self.valid_metrics = {\n",
    "            'avg_loss': [],\n",
    "            'accuracy': [],\n",
    "        }\n",
    "        self.counter = 0\n",
    "        self.delta = config.early_stopping_delta\n",
    "      \n",
    "    def run(self):\n",
    "        self.model.to(device)\n",
    "        best_valid_loss = float('inf')\n",
    "        best_valid_auc_mean = 0\n",
    "\n",
    "        try:\n",
    "            for i_epoch in tqdm(range(self.config.num_epochs), desc='Epochs', total=config.num_epochs, position=1, leave=True):\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_loss, train_outputs, train_targets = self._train()\n",
    "                valid_loss, valid_outputs, valid_targets = self._evaluate()\n",
    "                    \n",
    "                self.train_metrics[\"avg_loss\"].append(train_loss)\n",
    "                self.train_metrics[\"accuracy\"].append(self.comp_metric(train_outputs, train_targets))\n",
    "                \n",
    "                self.valid_metrics[\"avg_loss\"].append(valid_loss)\n",
    "                self.valid_metrics[\"accuracy\"].append(self.comp_metric(valid_outputs, valid_targets))\n",
    "                \n",
    "                end_time = time.time()\n",
    "                epoch_mins, epoch_secs = self._epoch_time(start_time, end_time)\n",
    "                self.print_progress(i_epoch, epoch_mins, epoch_secs)\n",
    "                \n",
    "                self.scheduler.step(self.valid_metrics[\"accuracy\"][-1])\n",
    "                \n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    torch.save(model.state_dict(), f\"{config.save_dirname}/best_model_epoch={i_epoch+1}.pth\")\n",
    "                    \n",
    "                if self.valid_metrics[\"accuracy\"][-1] > best_valid_auc_mean:\n",
    "                    self.counter = 0\n",
    "                    best_valid_auc_mean = self.valid_metrics[\"accuracy\"][-1]\n",
    "                    torch.save(model.state_dict(), f\"{config.save_dirname}/best_model_epoch={i_epoch+1}_auc_mean={best_valid_auc_mean}.pth\")\n",
    "                else:\n",
    "                    self.counter += 1\n",
    "                    \n",
    "                if self.counter > self.config.patience:\n",
    "                    print(\"EarlyStopping\")\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        \n",
    "        return self.train_metrics, self.valid_metrics\n",
    "        \n",
    "    def _train(self):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_output = None\n",
    "        epoch_target = None\n",
    "        for i, (images, labels) in tqdm(enumerate(self.train_dataloader), desc='Train', total=len(self.train_dataloader), position=2, leave=True):\n",
    "            loss_iten, outputs = self._train_process(images, labels)\n",
    "            epoch_loss += loss_iten              \n",
    "\n",
    "            if epoch_output is None:\n",
    "                epoch_output = outputs.cpu().data\n",
    "            else:\n",
    "                epoch_output = torch.cat((epoch_output, outputs.cpu().data))\n",
    "\n",
    "            if epoch_target is None:\n",
    "                epoch_target = labels.cpu().data\n",
    "            else:\n",
    "                epoch_target = torch.cat((epoch_target, labels.cpu().data))\n",
    "            \n",
    "        return epoch_loss / len(self.train_dataloader), epoch_output, epoch_target\n",
    "    \n",
    "    def _train_process(self, images, labels):\n",
    "        images = images.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), outputs\n",
    "            \n",
    "    def _evaluate(self):\n",
    "        model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_output = None\n",
    "        epoch_target = None\n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(self.valid_dataloader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                if epoch_output is None:\n",
    "                    epoch_output = outputs.cpu().data\n",
    "                else:\n",
    "                    epoch_output = torch.cat((epoch_output, outputs.cpu().data))\n",
    "\n",
    "                if epoch_target is None:\n",
    "                    epoch_target = labels.cpu().data\n",
    "                else:\n",
    "                    epoch_target = torch.cat((epoch_target, labels.cpu().data))\n",
    "\n",
    "        return epoch_loss / len(self.valid_dataloader), epoch_output, epoch_target\n",
    " \n",
    "    def _epoch_time(self, start_time, end_time):\n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "\n",
    "    def print_progress(self, i_epoch, epoch_mins, epoch_secs):\n",
    "        i_epoch = i_epoch + 1\n",
    "        print(f\"Epoch: {i_epoch:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "        print(\"Training Results - Average Loss: {:.4f} | accuracy: {:.4f}\"\n",
    "            .format(\n",
    "                self.train_metrics['avg_loss'][-1], \n",
    "                self.train_metrics['accuracy'][-1],\n",
    "            ))\n",
    "        print(\"Evaluating Results - Average Loss: {:.4f} | accuracy: {:.4f}\"\n",
    "            .format( \n",
    "                self.valid_metrics['avg_loss'][-1], \n",
    "                self.valid_metrics['accuracy'][-1],\n",
    "            ))\n",
    "        print()\n",
    "        \n",
    "    def comp_metric(self, preds, targs, labels=range(config.num_classes)):\n",
    "        preds = torch.sigmoid(preds)\n",
    "        targs = torch.eye(config.num_classes)[targs]\n",
    "        return np.mean([accuracy_score(targs[:,i], preds[:,i].round(), normalize=False) for i in labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080cfa2a8bee4f48a7bbfcdb980f8a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=200.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cc238e2f2547afa90451d6408eece0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=279.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 01 | Time: 0m 13s\n",
      "Training Results - Average Loss: 4.2326 | accuracy: 2201.6452\n",
      "Evaluating Results - Average Loss: 3.6164 | accuracy: 549.8925\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4589ad578c4a03bc1bbaae89ab3997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=279.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 02 | Time: 0m 14s\n",
      "Training Results - Average Loss: 3.0854 | accuracy: 2205.6129\n",
      "Evaluating Results - Average Loss: 3.5302 | accuracy: 549.8280\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c725b522f4d34cbb89d70eaff36e72df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', max=279.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, device, config)\n",
    "trainer.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

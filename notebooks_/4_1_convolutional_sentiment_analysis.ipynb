{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import traceback\n",
    "import datetime\n",
    "import joblib\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigExperiment:\n",
    "    seed = 42\n",
    "    positive_file = \"../data/positive.csv\"\n",
    "    negative_file = \"../data/negative.csv\"\n",
    "    test_size = 0.3\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    embed_dim = 300\n",
    "    max_vocab_size = 50_000\n",
    "    batch_size = 64\n",
    "    num_epochs = 30\n",
    "    lr = 1e-2\n",
    "    num_workers = 0\n",
    "    patience = 3\n",
    "    early_stopping_delta = 1e-4\n",
    "    save_dirname = \"models\"\n",
    "    \n",
    "config = ConfigExperiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    \n",
    "init_random_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((226834, 2), (111923, 12), (114911, 12))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\"id\", \"tdate\", \"tmane\", \"ttext\", \"ttype\", \"trep\", \"trtw\", \"tfav\", \"tstcount\", \"tfoll\", \"tfrien\", \"listcount\"]\n",
    "positive_df = pd.read_csv(config.positive_file, sep=\";\", names=column_names, index_col=False)\n",
    "negative_df = pd.read_csv(config.negative_file, sep=\";\", names=column_names, index_col=False)\n",
    "\n",
    "# Смена метки класса для отрицательной эмоциональной окраски\n",
    "negative_df[\"ttype\"] = 0\n",
    "\n",
    "df = pd.concat([negative_df, positive_df])\n",
    "df = df[[\"ttext\", \"ttype\"]]\n",
    "df.columns = ['text', 'target']\n",
    "\n",
    "df.shape, negative_df.shape, positive_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на работе был полный пиддес :| и так каждое закрытие месяца, я же свихнусь так D:',\n",
       " 'Коллеги сидят рубятся в Urban terror, а я из-за долбанной винды не могу :(',\n",
       " '@elina_4post как говорят обещаного три года ждут...((',\n",
       " 'Желаю хорошего полёта и удачной посадки,я буду очень сильно скучать( http://t.co/jCLNzVNv3S',\n",
       " 'Обновил за каким-то лешим surf, теперь не работает простоплеер :(',\n",
       " 'Котёнка вчера носик разбила, плакала и расстраивалась :(',\n",
       " '@juliamayko @O_nika55 @and_Possum Зашли, а то он опять затихарился, я прямо физически страдаю, когда он долго молчит!(((',\n",
       " 'а вообще я не болею -  я не выздоравливаю :(',\n",
       " 'я микрофраза :( учимся срать кирпичами в режиме &amp;quot;нон-стоп&amp;quot; @niwoqisipapy',\n",
       " 'я хочу с тобой помириться , но сука я гордая и никогда этого не сделаю! (((',\n",
       " '@DNO_OKEANA_A3A3 @MOE_MOPE_A3A3 тебя ебет какие у меня фотки.я про твои молчу.и вообще ты хоть знаешь как ТП то выглядят...',\n",
       " 'Блин начали сниться сны( Не когда почти не снились ! А теперь можно сказать каждый день такое ! Мне это не нравиться ((((((((((',\n",
       " '@realVold На твоем месте, я бы сначала телек купил бы(',\n",
       " '@faraonbgr111 плохо боишься значит(( ты где? В мск бгр?',\n",
       " 'А я хотела... электромобиль =(\\nhttp://t.co/vgs9JeUPHz',\n",
       " 'Скоро увижу твои зеленые глаза в последний раз(((',\n",
       " 'Вот это да, докатился,по-оранжевел патриот! Чо деется? :(( RT @SapelSV: #Бабки объединят #КПРФ и Навального? http://t.co/kyJk4A79kv',\n",
       " 'Ну уже 12:25 ночииии:(((( а я ничего не сделалаллалалалалалалвцмдэмппдпктцдкиицкп',\n",
       " '@190299der я очень рада,но ты им разобьешь сердца:(\\nМне их  жалко епт\\nмне оч жалко Влада\\nА всех остальных нет\\n',\n",
       " 'Сегодня какой-то нехороший день у меня :-( Телефон наушники не опознает, у блога как-то умудрился шаблон сломать, да еще',\n",
       " 'думаю,на моем веку почта россии не поднимется со дна (((',\n",
       " 'Мне одному   напоминает дом-2? Путин делает реалити-шоу? O_o',\n",
       " 'ебааааа сейчас модуль будет по региональной экономике ((',\n",
       " '@kordalona_mars оооууу ты меня анфолловил чувак((',\n",
       " 'вот так я встречаю зиму:(( http://t.co/FjivXAGcR9',\n",
       " '@Kyo_oppa ну вот, туда медведь прикреплялся.',\n",
       " 'чорт... хотела сходить в кино...  придется любоваться фильмом в уг качестве на компе(',\n",
       " 'RT @arinaaaaonskul: Так плохо я себя не чувствовала давно :( ',\n",
       " 'Как понизить температуру ноута? Снаружи +30, так что он постоянно раскалённый :(',\n",
       " '@Anastasia_pain думаешь упаду со стула? :(',\n",
       " '@Deniska_perm да нет, но условия будут более спартанские :(',\n",
       " 'Хотела маме сделать сюрприз, как всегда все навернулось(',\n",
       " 'я блин сутками его жду а он с друзьями общаться! с которыми ежедневно видится! :(',\n",
       " '@Annya_Nik @YankaMurygina Так и думал:( \\nНе судьба походу попасть к мастерам',\n",
       " 'RT @davidmsk: У берегов Коста-Рики водятся 1600 видов рыб. А на берег выходят черепахи весом до 500 кг. Но я не видел :( http://t.co/9ixMBH…',\n",
       " '\"@lisi_Ca: @maybe_reg49_27 @One_ENCLAVE @LyudaLa самая смешная из лисичек - тибетская. Это не фотошоп http://t.co/kDDYZartwl\" о_О',\n",
       " 'Потеряла креапления от борда. Как так! Я же их не снимала(',\n",
       " 'RT @Ksenyamalischev: Устала от семинара;(',\n",
       " 'Опять простыла....!!!;(Все гуляют,катают,тусуют,а я!;(((Что за невезение!!!',\n",
       " '@1libertad первые деньги пойдут на красные мокасины, потом на ремень гермес, потом на приору, а потом уже можно умереть :-/']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    # Remove digits\n",
    "    text = re.sub(\"\\d+:\\d+\", \" \", text)\n",
    "    text = re.sub(\" \\d+\", \" \", text)\n",
    "    # Removing ';quot;'\n",
    "    text = re.sub(';quot;', '', text) \n",
    "    # Removing ';quot;'\n",
    "    text = re.sub('&amp', '', text) \n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    text = re.sub(r'\\&\\w*;', ' ', text)\n",
    "    #Convert @username to AT_USER\n",
    "    text = re.sub('@[^\\s]+','AT_USER', text)\n",
    "    #Removing @mentions\n",
    "    text = re.sub('@[A-Za-z0–9]+', '', text)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    # Removing '#' hash tag\n",
    "    text = re.sub('#', '', text) \n",
    "    # Removing RT\n",
    "    text = re.sub('rt[\\s]+', '', text) \n",
    "    # Removing hyperlink\n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text)\n",
    "    # Separate words and punctuation\n",
    "    text = re.findall(r\"[\\w']+|[.,!?;:()]\", text)\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the tweets\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на работе был полный пиддес : и так каждое закрытие месяца , я же свихнусь так d :',\n",
       " 'коллеги сидят рубятся в urban terror , а я из за долбанной винды не могу : (',\n",
       " 'AT_USER как говорят обещаного три года ждут . . . ( (',\n",
       " 'желаю хорошего полета и удачной посадки , я буду очень сильно скучать (',\n",
       " 'обновил за каким то лешим surf , теперь не работает простоплеер : (',\n",
       " 'котенка вчера носик разбила , плакала и расстраивалась : (',\n",
       " 'AT_USER AT_USER AT_USER зашли , а то он опять затихарился , я прямо физически страдаю , когда он долго молчит ! ( ( (',\n",
       " 'а вообще я не болею я не выздоравливаю : (',\n",
       " 'я микрофраза : ( учимся срать кирпичами в режиме нон стоп AT_USER',\n",
       " 'я хочу с тобой помириться , но сука я гордая и никогда этого не сделаю ! ( ( (',\n",
       " 'AT_USER AT_USER тебя ебет какие у меня фотки . я про твои молчу . и вообще ты хоть знаешь как тп то выглядят . . .',\n",
       " 'блин начали сниться сны ( не когда почти не снились ! а теперь можно сказать каждый день такое ! мне это не нравиться ( ( ( ( ( ( ( ( ( (',\n",
       " 'AT_USER на твоем месте , я бы сначала телек купил бы (',\n",
       " 'AT_USER плохо боишься значит ( ( ты где ? в мск бгр ?',\n",
       " 'а я хотела . . . электромобиль (',\n",
       " 'скоро увижу твои зеленые глаза в последний раз ( ( (',\n",
       " 'вот это да , докатился , по оранжевел патриот ! чо деется ? : ( ( AT_USER бабки объединят кпрф и навального ?',\n",
       " 'ну уже ночииии : ( ( ( ( а я ничего не сделалаллалалалалалалвцмдэмппдпктцдкиицкп',\n",
       " 'AT_USER я очень рада , но ты им разобьешь сердца : ( мне их жалко епт мне оч жалко влада а всех остальных нет',\n",
       " 'сегодня какой то нехороший день у меня : ( телефон наушники не опознает , у блога как то умудрился шаблон сломать , да еще',\n",
       " 'думаю , на моем веку почта россии не поднимется со дна ( ( (',\n",
       " 'мне одному напоминает дом 2 ? путин делает реалити шоу ? o_o',\n",
       " 'ебааааа сейчас модуль будет по региональной экономике ( (',\n",
       " 'AT_USER оооууу ты меня анфолловил чувак ( (',\n",
       " 'вот так я встречаю зиму : ( (',\n",
       " 'AT_USER ну вот , туда медведь прикреплялся .',\n",
       " 'чорт . . . хотела сходить в кино . . . придется любоваться фильмом в уг качестве на компе (',\n",
       " 'AT_USER так плохо я себя не чувствовала давно : (',\n",
       " 'как понизить температуру ноута ? снаружи 30 , так что он постоянно раскаленный : (',\n",
       " 'AT_USER думаешь упаду со стула ? : (',\n",
       " 'AT_USER да нет , но условия будут более спартанские : (',\n",
       " 'хотела маме сделать сюрприз , как всегда все навернулось (',\n",
       " 'я блин сутками его жду а он с друзьями общаться ! с которыми ежедневно видится ! : (',\n",
       " 'AT_USER AT_USER так и думал : ( не судьба походу попасть к мастерам',\n",
       " 'AT_USER у берегов коста рики водятся видов рыб . а на берег выходят черепахи весом до кг . но я не видел : (',\n",
       " 'AT_USER AT_USER AT_USER AT_USER самая смешная из лисичек тибетская . это не фотошоп о_о',\n",
       " 'потеряла креапления от борда . как так ! я же их не снимала (',\n",
       " 'AT_USER устала от семинара ; (',\n",
       " 'опять простыла . . . . ! ! ! ; ( все гуляют , катают , тусуют , а я ! ; ( ( ( что за невезение ! ! !',\n",
       " 'AT_USER первые деньги пойдут на красные мокасины , потом на ремень гермес , потом на приору , а потом уже можно умереть :']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Фильтруем по части речи и возвращаем только начальную форму.\n",
    "def lemmatize(text):\n",
    "    words = []\n",
    "    for token in text.split():\n",
    "        # Если токен уже был закеширован, быстро возьмем результат из кэша.\n",
    "        if token in cache.keys():\n",
    "            words.append(cache[token])\n",
    "        # Слово еще не встретилось, будем проводить медленный морфологический анализ.\n",
    "        else:\n",
    "            result = morph.parse(token)   \n",
    "            word = result[0].normal_form\n",
    "            # Отправляем слово в результат, ...\n",
    "            words.append(word)\n",
    "            # ... и кешируем результат его разбора.\n",
    "            cache[token] = word   \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.3 s, sys: 110 ms, total: 40.4 s\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на работа быть полный пиддес : и так каждый закрытие месяц , я же свихнуться так d :',\n",
       " 'коллега сидеть рубиться в urban terror , а я из за долбать винд не мочь : (',\n",
       " 'at_user как говорят обещаной три год ждать . . . ( (',\n",
       " 'желать хороший полёт и удачный посадка , я быть очень сильно скучать (',\n",
       " 'обновить за какой то леший surf , теперь не работать простоплеер : (',\n",
       " 'котёнок вчера носик разбить , плакать и расстраиваться : (',\n",
       " 'at_user at_user at_user заслать , а то он опять затихариться , я прямо физически страдать , когда он долго молчать ! ( ( (',\n",
       " 'а вообще я не болеть я не выздоравливать : (',\n",
       " 'я микрофраза : ( учиться срать кирпич в режим нона стоп at_user',\n",
       " 'я хотеть с ты помириться , но сук я гордый и никогда это не сделать ! ( ( (',\n",
       " 'at_user at_user ты ебета какой у я фотка . я про твой молчать . и вообще ты хоть знаешь как тп то выглядеть . . .',\n",
       " 'блин начать сниться сон ( не когда почти не сниться ! а теперь можно сказать каждый день такой ! я это не нравиться ( ( ( ( ( ( ( ( ( (',\n",
       " 'at_user на твой место , я бы сначала телек купить бы (',\n",
       " 'at_user плохо бояться значит ( ( ты где ? в мск бгр ?',\n",
       " 'а я хотеть . . . электромобиль (',\n",
       " 'скоро увидеть твой зелёный глаз в последний раз ( ( (',\n",
       " 'вот это да , докатиться , по оранжевело патриот ! чо деяться ? : ( ( at_user бабка объединить кпрф и навальный ?',\n",
       " 'ну уже ночииия : ( ( ( ( а я ничто не сделалаллалалалалалалвцмдэмппдпктцдкиицкп',\n",
       " 'at_user я очень рада , но ты имя разбить сердце : ( я они жалко епт я оч жалко влад а весь остальной нет',\n",
       " 'сегодня какой то нехороший день у я : ( телефон наушник не опознать , у блог как то умудриться шаблон сломать , да ещё',\n",
       " 'думать , на мыть век почта россия не подняться с дно ( ( (',\n",
       " 'я один напоминать дом 2 ? путин делать реалить шоу ? o_o',\n",
       " 'ебааааа сейчас модуль быть по региональный экономика ( (',\n",
       " 'at_user оооууа ты я анфолловить чувак ( (',\n",
       " 'вот так я встречать зима : ( (',\n",
       " 'at_user ну вот , туда медведь прикрепляться .',\n",
       " 'чорт . . . хотеть сходить в кино . . . прийтись любоваться фильм в уг качество на комп (',\n",
       " 'at_user так плохо я себя не чувствовать давно : (',\n",
       " 'как понизить температура ноут ? снаружи 30 , так что он постоянно раскалить : (',\n",
       " 'at_user думать упад с стул ? : (',\n",
       " 'at_user да нет , но условие быть более спартанский : (',\n",
       " 'хотеть мама сделать сюрприз , как всегда весь навернуться (',\n",
       " 'я блин сутки он ждать а он с друг общаться ! с который ежедневно видеться ! : (',\n",
       " 'at_user at_user так и думать : ( не судьба поход попасть к мастер',\n",
       " 'at_user у берег коста рик водиться вид рыба . а на берег выходить черепаха вес до килограмм . но я не видеть : (',\n",
       " 'at_user at_user at_user at_user самый смешной из лисичка тибетский . это не фотошоп о_о',\n",
       " 'потерять креапление от борд . как так ! я же они не снимать (',\n",
       " 'at_user устать от семинар ; (',\n",
       " 'опять простыть . . . . ! ! ! ; ( весь гулять , катать , тусовать , а я ! ; ( ( ( что за невезение ! ! !',\n",
       " 'at_user один деньга пойти на красный мокасин , потом на ремень гермес , потом на приор , а потом уже можно умереть :']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].tolist()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136100, 2), (45367, 2), (45367, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Формирование train test valid данных\n",
    "\n",
    "# df = pd.read_csv(\"../data/preprocessed_text_v1.csv\", index_col=False)\n",
    "\n",
    "df = df.drop(df[df['text'].map(str) == 'nan'].index)\n",
    "\n",
    "train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "train.to_csv(\"../data/train_processed_data.csv\", index=False)\n",
    "validate.to_csv(\"../data/validate_processed_data.csv\", index=False)\n",
    "test.to_csv(\"../data/test_processed_data.csv\", index=False)\n",
    "\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: str(x).split()\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenize, batch_first=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "fields = [('text',TEXT), ('label', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                        path=\"../data/\",\n",
    "                                        train=\"train_processed_data.csv\",\n",
    "                                        validation=\"validate_processed_data.csv\",\n",
    "                                        test=\"test_processed_data.csv\",\n",
    "                                        format=\"csv\",\n",
    "                                        fields=fields,\n",
    "                                        skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['оставаться', 'самый', 'нужный', 'и', 'самый', 'близкие', ')', 'весь', 'остальной', 'уходить', ')', 'и', 'я', 'только', 'рада', ')', 'потому', 'что', 'я', 'никогда', 'сам', 'не', 'понять', 'нужный', 'я', 'человек', 'или', 'нет', ')'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['at_user', 'at_user', 'я', 'старушка', '(', '(', '('], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(valid_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['at_user', 'привееть', ',', 'хелена', ':', ')', ')', ')', 'мимими', '.', '.', '.', 'пряник', ',', 'конфета', 'и', 'сирец'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 136100\n",
      "Number of validation examples: 45367\n",
      "Number of testing examples: 45367\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT.build_vocab(train_data, max_size=config.max_vocab_size)\n",
    "TEXT.build_vocab(train_data, min_freq=2)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 33015\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    sort_key = lambda x: x.text,\n",
    "    batch_size=config.batch_size,\n",
    "    device=config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Train:')\n",
    "# for batch in train_iterator:\n",
    "#     print(batch)\n",
    "    \n",
    "# print('Valid:')\n",
    "# for batch in valid_iterator:\n",
    "#     print(batch)\n",
    "    \n",
    "# print('Test:')\n",
    "# for batch in test_iterator:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', 126857), (')', 116605), (',', 112838), ('.', 110646), ('at_user', 89459), (':', 64837), ('я', 64791), ('не', 44975), ('!', 39989), ('и', 36467), ('в', 35466), ('что', 23049), ('на', 22897), ('а', 21841), ('?', 21683), ('с', 20052), ('весь', 18848), ('ты', 18175), ('быть', 16970), ('это', 14896)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '(', ')', ',', '.', 'at_user', ':', 'я', 'не']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33015, 300])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import gensim\n",
    "import wget\n",
    "\n",
    "model_url = 'http://vectors.nlpl.eu/repository/11/187.zip'\n",
    "# wget.download(model_url)\n",
    "w2v_model = gensim.models.KeyedVectors.load('187/model.model')\n",
    "numpy_embeddings = np.zeros(shape=[len(TEXT.vocab), config.embed_dim],dtype=np.float32)\n",
    "\n",
    "for word in TEXT.vocab.itos:\n",
    "    vector = w2v_model.get_vector(word)\n",
    "    index  = TEXT.vocab.stoi[word]\n",
    "    numpy_embeddings[index] = vector\n",
    "    \n",
    "pretrained_embeddings = torch.Tensor(numpy_embeddings).float()\n",
    "pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = embedding_dim, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        \n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = config.embed_dim\n",
    "N_FILTERS = 128\n",
    "FILTER_SIZES = [2, 3]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,097,013 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0550e-02, -3.5227e-01, -1.5996e-01,  ..., -6.7555e-01,\n",
       "          1.6213e-01, -6.4838e-01],\n",
       "        [ 5.5938e-01,  7.1610e-01, -5.2220e-02,  ..., -3.1100e-01,\n",
       "          1.0994e+00, -7.4458e-01],\n",
       "        [ 8.6657e-04,  2.8588e-03, -1.2546e-03,  ...,  2.7372e-03,\n",
       "          3.3164e-03,  2.3352e-03],\n",
       "        ...,\n",
       "        [ 1.2707e-01, -2.0811e-02,  1.6323e-01,  ..., -1.2492e-03,\n",
       "          2.0801e-01, -2.6840e-03],\n",
       "        [-1.0621e+00,  2.0553e+00, -1.1423e+00,  ...,  3.4679e-01,\n",
       "         -3.4598e-01, -1.1714e+00],\n",
       "        [-2.3426e-02,  7.1213e-02, -9.7777e-02,  ...,  1.2438e-04,\n",
       "         -1.1894e-03,  2.1802e-02]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(config.device)\n",
    "criterion = criterion.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.124 | Train Acc: 94.01%\n",
      "\t Val. Loss: 0.017 |  Val. Acc: 99.24%\n",
      "Epoch: 02 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.020 | Train Acc: 99.24%\n",
      "\t Val. Loss: 0.016 |  Val. Acc: 99.34%\n",
      "Epoch: 03 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.35%\n",
      "\t Val. Loss: 0.017 |  Val. Acc: 99.32%\n",
      "Epoch: 04 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.49%\n",
      "\t Val. Loss: 0.025 |  Val. Acc: 99.24%\n",
      "Epoch: 05 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.014 | Train Acc: 99.55%\n",
      "\t Val. Loss: 0.024 |  Val. Acc: 99.32%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.016 | Test Acc: 99.34%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN2d \n",
    "# Test Loss: 0.371 | Test Acc: 84.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CNN1d \n",
    "# Test Loss: 0.333 | Test Acc: 85.73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

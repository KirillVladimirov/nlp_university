{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader,\n",
    "epochs=20, device, mix_loader):\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in zip(train_loader,mix_loader):\n",
    "      ((inputs, targets),(inputs_mix, targets_mix)) = batch\n",
    "      optimizer.zero_grad()\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "      inputs_mix = inputs_mix.to(device)\n",
    "      target_mix = targets_mix.to(device)\n",
    "\n",
    "      distribution = torch.distributions.beta.Beta(0.5,0.5)\n",
    "      beta = distribution.expand(torch.zeros(batch_size).shape).sample().to(device)\n",
    "\n",
    "      # We need to transform the shape of beta\n",
    "      # to be in the same dimensions as our input tensor\n",
    "      # [batch_size, channels, height, width]\n",
    "\n",
    "      mixup = beta[:, None, None, None]\n",
    "\n",
    "      inputs_mixed = (mixup * inputs) + (1-mixup * inputs_mix)\n",
    "\n",
    "      # Targets are mixed using beta as they have the same shape\n",
    "\n",
    "      targets_mixed = (beta * targets) + (1-beta * inputs_mix)\n",
    "\n",
    "      output_mixed = model(inputs_mixed)\n",
    "\n",
    "      # Multiply losses by beta and 1-beta,\n",
    "      # sum and get average of the two mixed losses\n",
    "\n",
    "      loss = (loss_fn(output, targets) * beta\n",
    "             + loss_fn(output, targets_mixed)\n",
    "             * (1-beta)).mean()\n",
    "\n",
    "      # Training method is as normal from herein on\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        super(LabelSmoothingCrossEntropyLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        num_classes = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = (-log_preds.sum(dim=-1)).mean()\n",
    "        nll = F.nll_loss(log_preds, target)\n",
    "        final_loss = self.epsilon * loss / num_classes +\n",
    "                     (1-self.epsilon) * nll\n",
    "        return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logger\n",
    "\n",
    "from pytorch_transformers.tokenization import BertTokenizer\n",
    "from fast_bert.data import BertDataBunch\n",
    "from fast_bert.learner import BertLearner\n",
    "from fast_bert.metrics import accuracy\n",
    "\n",
    "device = torch.device('cuda')\n",
    "logger = logging.getLogger()\n",
    "metrics = [{'name': 'accuracy', 'function': accuracy}]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained\n",
    "                ('bert-base-uncased',\n",
    "                  do_lower_case=True)\n",
    "\n",
    "\n",
    "databunch = BertDataBunch([PATH_TO_DATA],\n",
    "                          [PATH_TO_LABELS],\n",
    "                          tokenizer,\n",
    "                          train_file=[TRAIN_CSV],\n",
    "                          val_file=[VAL_CSV],\n",
    "                          test_data=[TEST_CSV],\n",
    "                          text_col=[TEST_FEATURE_COL], label_col=[0],\n",
    "                          bs=64,\n",
    "                          maxlen=140,\n",
    "                          multi_gpu=False,\n",
    "                          multi_label=False)\n",
    "\n",
    "\n",
    "learner = BertLearner.from_pretrained_model(databunch,\n",
    "                      'bert-base-uncased',\n",
    "                      metrics,\n",
    "                      device,\n",
    "                      logger,\n",
    "                      is_fp16=False,\n",
    "                      multi_gpu=False,\n",
    "                      multi_label=False)\n",
    "\n",
    "learner.fit(3, lr='1e-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
